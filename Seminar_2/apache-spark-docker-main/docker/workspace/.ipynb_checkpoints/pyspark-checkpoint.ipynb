{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: hive.metastore.uris\n",
      "24/10/12 19:11:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\r\n",
    "from pyspark.sql.functions import to_json,col\r\n",
    "from pyspark.sql.types import *\r\n",
    "from os.path import abspath\r\n",
    "\r\n",
    "spark = SparkSession\\\r\n",
    "        .builder\\\r\n",
    "        .appName(\"pyspark-notebook\")\\\r\n",
    "        .master(\"spark://spark-master:7077\")\\\r\n",
    "        .config(\"spark.executor.memory\", \"512m\")\\\r\n",
    "        .config(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\")\\\r\n",
    "        .config(\"spark.sql.warehouse.dir\", \"/user/hive/warehouse\")\\\r\n",
    "        .enableHiveSupport()\\\r\n",
    "        .getOrCreate()\r\n",
    "\r\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "wget is already the newest version (1.20.1-1.1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!apt install wget -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-10-12 19:19:38--  https://drive.google.com/uc?export=download&id=1wdytBkoYRyaNNv3XUNnwcGbdNUSfok7u\n",
      "Resolving drive.google.com (drive.google.com)... 209.85.233.194, 2a00:1450:4010:c03::c2\n",
      "Connecting to drive.google.com (drive.google.com)|209.85.233.194|:443... connected.\n",
      "HTTP request sent, awaiting response... 303 See Other\n",
      "Location: https://drive.usercontent.google.com/download?id=1wdytBkoYRyaNNv3XUNnwcGbdNUSfok7u&export=download [following]\n",
      "--2024-10-12 19:19:39--  https://drive.usercontent.google.com/download?id=1wdytBkoYRyaNNv3XUNnwcGbdNUSfok7u&export=download\n",
      "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 142.250.74.65, 2a00:1450:400f:802::2001\n",
      "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|142.250.74.65|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 8886971 (8.5M) [application/octet-stream]\n",
      "Saving to: ‘uplift_data.zip’\n",
      "\n",
      "uplift_data.zip     100%[===================>]   8.47M  2.63MB/s    in 3.2s    \n",
      "\n",
      "2024-10-12 19:19:48 (2.63 MB/s) - ‘uplift_data.zip’ saved [8886971/8886971]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget 'https://drive.google.com/uc?export=download&id=1wdytBkoYRyaNNv3XUNnwcGbdNUSfok7u' -O uplift_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  uplift_data.zip\n",
      "  inflating: clients.csv             \n",
      "  inflating: uplift_test.csv         \n",
      "  inflating: uplift_train.csv        \n"
     ]
    }
   ],
   "source": [
    "!unzip uplift_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None         \u001b[0m\u001b[01;32mpyspark.ipynb\u001b[0m*   uplift_test.csv\n",
      "clients.csv  uplift_data.zip  uplift_train.csv\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv(path=\"clients.csv\", sep=\",\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400162"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(client_id='000012768d', first_issue_date='2017-08-05 15:40:48', first_redeem_date='2018-01-04 19:30:07', age='45', gender='U')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = data.select([\"client_id\", \"age\", \"gender\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(client_id='000012768d', age='45', gender='U')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = data.select(['client_id', 'age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "co3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.write.saveAsTable('hive_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf= spark.sql(\"\"\"\r\n",
    "select * from hive_table\r\n",
    "\"\"\")\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
